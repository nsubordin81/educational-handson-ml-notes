# My Notes From The Deep Q Learning Paper

## Notes To Self - This is Too Much A Verbatim From The Paper, You Won't Remember It, Try Again In Your Own Words Later

key motivation, even though reinforcement learning theory works for agents in simple environments, it doesn't work as well when the agent needs to try to get control over a more realistic or complex environment. 

The agent's job gets harder, they have to make an efficient representation of the environment out of high dimensional s3ensory inputs and use that to generalize past experience to new situations. 

It turns out that animals and human agents solve control problems by using both reinforcement learning and also hierarchical sensory processing systems. we have a lot of data about the nervous systems of animals that shows parallels to the phasic signals that dopaminergic neurons emit and reinforcement learning algorithms specifically temporal difference. 

a limitation of using reinforcement learning agents to solve control problems has been they need domains where you can feasibly handcraft your features or you have state spaces that are both fully observed or low dimensional vs. continuous or infinite. 

Deep Q Networks learn from high dimensional sensory inputs and use end to end reinforcement learning. Atari 2600 games are the subject, the inputs are the pixels and the game score, and with just these two things the agent was able to beat out the performance of all prior algorithms and get to a level comparable to a professional human games tester. The Deep Q Network used to achieve this was used across 49 different games and the algorithm, network architecture and hyperparameters were able to stay the same. So you could say the deep q network is the first bridge between high dimensional inputs and the resulting actions an agent should take in the environment. this makes an agent that is capable of learning to e cel at a diverse array of challenging tasks. 

deep nueral nets have been getting more layers of nodes within them so they have more ways to represent the data that is coming in with sets of abstractions. deep convolutional neural nets are one of the ones that are used in deep q networks because they are capable of taking hierarchical layers of tiled filters to mimic the receptive fields that we use for vision, the 'feed-forward' processing in the visual cortex. this takes advantage of the fact that there are local spatial correlations in images and conv nets are also able to hands changes with the point of view of the image or the scale of it. 

For deep q the situation is the same as with reinforcement learning. an agent interacts with an enviornment and that is shown through observations, actions and rewards. The agent is trying to maximize their cumulative future reward. The goal is to use the deep convolutional neural network to approximate the optimal action-value function

<place to get the mathematical notation for the action value function.> 

This action value function is the maximum sum over the rewards that have been discounted by gamma at each  time step t. So for every time step you earn a reward and then for each reward you apply a discount because that reward came in the future. So say there was some behavioral policy pi that you were following, it makes observations and then recommends actions and those actions lead to rewards and transitions between states. You really want to get the most reward you can get across all the actions you can take while acting in the environment, but the actions you take in the future will be of less value than the ones you'll take right away so they have some discount. 
 
 there have been issues with reinforcement learning because it becomes unstable or divergent when a nerual network is used to approximate the action value function. This is also true for other non linear approximators. There are many reasons for this: 
	1. there are correlations present in the sequence of observations
	2. small updates to the action value function itself can sifnificantly change the policy and therefore the data distribution itself. 
	3. there are correlations between the action-values and also the target values represented by the reward plus the gamma discounted value of the maximum action value for the following state. 

There is a novel varieat of q learning introduced in the paper which addresses the instabilites found with these other nonlinear attempts. 

	1. Experience replay - this randomizes over the data, the observations don't get processed in the order in which they temporally appear. This helps get rid of issues with sequantial correlations between observations and it smoothes over the changes in the data distribution because actions don't get amplified because they are similar and happen together. 
	2. then there is the fixed parameter update which keeps some target values fixed and only updates them every once in a while, and then has the update adjust the action values towards those slowly changing targets. this will reduce correlations between the action value and the target action value. 

there are other stable applications of neural network s to reinforcement learning, one is called neural fitted q iteration. abut this method requires you to repeatedly train the networks from scratch on hundrends of iterations. they are too inefficient to be used successfully with large neural networks. 

the approach taken by deep q learning is to parameterize the approximate value function q(s, a; thea). this is done with the deep convolutional neural network  and the thea are the parameters, the weights if you will of the q network at ech iteration. to perform the experience replay the experiences are stored in an experiance tuple called et and consist of the state, the action, the reward, and then the following state. one of these such tuples is stored for each time step in a dataset Dt which has e1 through et inside. while learning there are q learning updates applied on some minibatches of experience (so some group of timestep together) and those are drawn from the Dt dataset uniformly and at random to remove the chance for correlation. 

updating the q learning for each iteration of the network (each time it processes one of these minibatches of experiences) is done through a loss function given by 

<show the loss function>

so that e means it si the expected value in terms of the state, action, reward, next state tuples drawn with uniform random distribution from Dt
of the square of the quantity given by
the difference between
the sum of the reward and the approximate value function of the following timestep using fixed theta and 
the approximate value of the current state and action with a non fixed theta. 

it is a difference because the value of the reward combined with the action value of the next step is standing in for the optimal action value. it is reward the agent stands to gain, and the amount of value that is currently had is the other term. So we are essentially saying, "you haven't got all the reward yet, try to minimize how far off you are from getting that reward."

we square be cause this is the squared error and this is a convenient mathematical trick which helps the data be more normal and also not negative

the data transformations required to make the deep q network work were impressive in their own right. so they started with high dimensional data that was 210 by 160 resolution color video runnin at 60 frames per second and they converted it to 84 by 84 by 4 microbatches of square images. 

the neural network learning agent was provided with only the visual images experienced by the game and the range of actions available to the agent for each game, but not the ways in which the actions would affect the visuals, as in how you could use the actions to make the things you wanted to happen on the screen happen or even what that was. 

there were two indices of learning used to measure the progress of the agent. first there was an average score per episode maintained for the agent and the second part was an average predicted q value. 

the performance of the deep q network is undeniably good. it outperformed the best other neural network based reinforcement learning approaches that had been measured on the atari games and then it also performed consistently well as the human equivalent on many of the games. 

to understand why the DQN does so well the team that created it looked at the representations that it learned when performing on the space invaders game. There is a technique used called t-SNE. this algorithm maps the DQN representations of states that we can perceive as similar to nearby points on a visual ization. the algorithm also was aable to generate similar embeddings for a dqfn state that are similar in terms of the reward they can provide but don' tlook similar to human viewers of the game. this tsne visualization helped confirm that the network was taking in high dimensional data and then figuring out a way to make adaptive behavioural decisions from it. It also turns out that the dqn representations aren't limited to data that has come about through solely its own policy for behavior. other agent and human player game states were fed in and the representations from the last hidden layer were recorded and put into tSNE. 

deep q networks were shown to exhibit some amount of mid to longer term strategy such as in breakout where they figured out the optimal strategy was to get the ball hidden behind all the bricks. However, a really longer term temporal strategy like one that requires taking no action or a few vvery specific actions that don't yield their value until a long way off, those are still a big challenge for deep q networks. 

the deep q network presented in the paper draws on the biological evidence that as an animal is engaging in perceptual learning if it gets reward signals then that may influence the characteristics of representations within the visual cortext. In the same way, the rewards from the reinforcement learning cause the convolutional neural net to figure out additional representations of the enviornment tha tit finds useful to maximizing values. The replay algorithm is also evolutionalrily based and was critical to getting the results tha the deep q learning approach had. In humans and mammals the hippocampus might actually be responsible for what the replay algorithm is doing, storing and representing the recently experienced transitions. When we rest or our brains are idling after some experience, the hippocampus will siometimes trigger the reactivation of recently experienced learned behaviors. Then 'value functions' are updated as the basal ganglia interacts with this hippocampal phenomena and that cases updates to optimize the experience to a new reward or goal. 

## Methods 

preprocessing - the input was frames from atari games, every frame could have 210 vertical pixels by 160 horizontal ones. Then there are 128 different colors that each of the pixels could be, so the input space for just one of these frames was 210x160x128. That means a lot of memory to store the values and lots of computation added for every transformation. first there were steps taken to deal with atari's pixel flickering issue. Atari games, being old and having limitations on the hardware for how many sprites it could paint per frame, sometimes accounted for this by not painting sprites every frame but instead every other frame. so there is a max() operation done between every two sequential frames to capture sprites that might have been erased between frames. 

There is a step also to remove the luminance channel from the RGB color frame, I guess because that doesn't provide very much value for the extra representation it requires, and then the width and height are rescaled to 84x84. there is a function that applies these prprocessing steps to m most recent frames from the gameplay and then stack them to create the input that the Q function uses. The m used for the deep q network that was published was 4 but it didn' thave to be, you could choose different values of m. 

you can view the cource code for the dqn on https://sites.google.com/a/deepmind.com/dqn

model architecture - There is a very important point made here and I didn't get it the first time I read it. They mention that there are different ways to parameterize a Q network, and that the way that was most common before they made theirs was to have both the state space and one selected action from the action space as input and then a scalar value representing the Q value for that action as the output. They then explain that you have to do a forward pass through the neural network for every action value in the action space on every state. that means you have a cost that scales linearly with the size of your action space. 

their workaround for this was to have a different neural net architecture that took in just the state representation and not the action and then instead of just outputting a single scalar value for the actions, it outputs a vector of values, one for each possible action, and then the agent can choose to use the one that has the highest value for exploitation or at randomly for exploration. Only one forward pass required. 

Some more architecure details described in plain english to the extent possible. you start with the 84x84x4 input tensor that comes from the preprocessor and represents 4 frames of the game. then there is a hidden layer next which convolves the 32 filters of 8x8 with a stride of 4, I need to review the convnet lectures from my other course to make sure I understand that, and you apply a recitfier nonlinearity for activation. then you use 64 filters in another hidden layer, the filters are smaller, only 4x4, and your stride is shorter too this time with only 2, and you also use ReLU? for activation. then there is one more convolution layer that has 64 filters 3x3 with stride 1 followed by a rectivfier. At the and of all that you have a fully connected hidden layer with 512 rectifier units. the final output layer after the last hidden layer is a fully connected layer that will give an output for each of the valid actions that can be taken. For info, on the 49 atari games this network was first applied to, there were a variety of numbers of actions ranging from 4 to 18.

training details - the deep q network was purposefully only used on the 49 games that already had other methods being practiced on them so there could be a basis for performance comparison. 

It is important to the team building the deep q network that they showed that even though they didn't use the same network to train for all the games, they did keep the architecture, learning algorithm, and the hyperparameters constant, so it was essentially a copy of the same network that they were training each time. 

While there was no true modification to the games when the trained networks were playing them, they did have to make a modification to the reward structure of the games while training them. this was to scale the scores of the games because in some games the scores were high values and others lower, so tehy were normalized and clipped to a range between -1 and 1. This helped to limit the scale of the error derivatives and not have to adjust alpha for different games. However, this means if the agent was used in an environment where the scores were not within this range it may not perform well. The network also took as input the number of lives remaining that the atari emulator emits, and this was used to determine when an episode was over. 

For the experiments they used RMSProp. this is a form of mini batch gradient descent. They provided some slides on it. So some of the challenges to stochastic gradient descent convergence, the gradient that is calculated has different values to adjust each of the weights by. Also, the amount of adjustment to each weight varies as the learning process continues. You want to be able to choose a single global learning rate over the whole network so you'd rather these weight adjustments were more incremental in nature and had constant rates of change across them. The only too you have when you learn the full batch of observations at once is to use the sign of the gradient. you can't change the magnitude of the weight updates you apply. There is a method called rprop which keeps the idea from full batch learning of changing the gradient sign, and combines that with changing the step size for a weight by multiplying it by a signed constant. instead of having the step size be a constant 1 for all the weights, they could be any size, but recommendation is to be less than 50 and more than 0.0001. when the learning rate is small you average the gradients over sucessive mini batches, that is the stochastic gradient descent. you want the weight not to be affected by encountering one stray gradient value if the rest are fairly consistent. rprop though doesn't do this right, because across the time steps if the same value was encountered multiple times it would have a stronger effect on the weight. example, say the gradient was +1 for nine successive mini batches and then there was one gradient that was -0.9. Instead of recognizing that this -0.9 means that the other adjustments are likely coincidental and the weight hasn't stabilized yet, the rprop method will add the gradients to the weight proportionate to their occurrence, which means that weight will be increased 9 times and decreased 1 time when it should not be increased so much. 

So rms prop seeks to give the advantage of adjusting timesteps that rprop provides for robustness, along with still having minibatches like stochastic gradient descent uses to efficiently converge to a local maximum (minimum? I forget), and then finally average the gradient values over successive mini batches effectively so that the weights don't grow or shrink too much in response to the gradient signal. 

rprop uses the gradient but the gradient that it uses is also scaled by the size of the gradient. the problem with this is that the gradient is a different size for each mini batch the model sees. so the insight of rms prop is to instead of scaling by a variable term like the gradient, scale by something that is a moving average of the squared gradient for every weight. that is accomplished by having 90 percent of the mean square gradient by the mean square from the last mini batch and then 10 percent be the new gradient with respect to the current minibatch . you then take the square root of that mean square term and you have something that is scaled consistently and will not allow the weights to grow or shrink too much like rprop does. 

there is some guidance about whether or not it is necessary to use rmsprop or rmsprop with momentum or lecun's model ro something else that has surfaced in the last 5 years or so since this paper was written. generally it says use the full batch method for small data sets or data sets that don't have data that is the same a lot. For larger, redundant data sets though the recommendation is to use rmsprop. 

the policy for behavior was epsilon greedy during training. and this epsilon value was annealed linearly from 1 to .1 over a million frames (I guess /4 so 250,000) and then it was fixed once it reached 0.1. I don't know what constant factor that would put the decay of epsilon at but you could figure it out with some algebra. is it 0.9999996? hmm, maybe, math is hard. Once epsilon stabilizes at .1 the agent continues for 49 million more frames, which they did the math for us, total time training is the equivalent of playing the atari game for 38 days straight (without pauses for water or bathroom breaks haha lucky machines). The experiential replay memory was 1 million frames, which we will probalby get to later. 

there is frame skipping trick used in the domain of machine learning for atari games. instead of selecting actions in every single frame, the agent allows the emulator to run forward k frames before selecting an action. going from frame to frame is much less expensive than the compute required to select an action and so when k is 4 that is a 4x improvement in performance. The agent can get through that many more games and the runtime overhead stays the same. 

hyperparameters and optimization params were arrived at through informal search using pong breakout, seaquest, spac3e invaders and beam rider. There could have been a grid search done to find the best performing combination of hyperparameters over a range of values but this was not opted for because it would be computationally very expensive. Once arrived at, the hyperparameters stayed the same for all other games. 

experimental setup was to have the minimal prior knowledge for the agent. the agent wasn't given access to the internal state of the emulator, only things that a human would be able to ascertain, such as an image representing the current frame of play, the score of the game (without modifying it for the test split as it was modified during training), the number of lives the agent had left in the game, and the number of available actions without describing to the algorithm in any way what effects those actions would have (like "this button makes your character move in an upward direction"). 

### Evaluation Procedure

agents played each game 30 times for 5 minutes at a time during evaluation. initial state of the game was determined from a random seed. and there was an epsilon greedy policy used where epsilon was fixed at a value of 0.05. that helps reduce the chance that an agent won't explore enough. The baseline to beat for evaluation was an agent acting randomly in the space. there was an action chosen at random by the baseline agent at a rate of 10 times per second or every 6th frame. that is becausae the fastest  human player would be able to press the 'fire' button about this many times, so for a handful of games it was necessary to limit the speed at which the agent could execute actions in case high performance could be achieved through extremely fast button mashing. If a random agent was allowed to select a random action every frame, then this had the effect of making the DQN normalized performance higher across six specific games by about 5% and it made it outperform the human tester in these 6 games by a lot (just because the ability to do random actions from a small subset of actions really fast is more valuable than thinking about and executing the right action).  

human testers used the same engine for emulating atari games that the agents did and they played in conditions that were similar for example they weren't permitted to ppause their game or reload from a saved state. the emulator was set to the frequency that an old atari game would ahve run at, the CRT tv standard of 60 frames per second. there was no sound feedback to either human or agent players. the human performance was measured as the average rewards achieved from 20 episodes of the game that were ach 5 minutes long after having practiced for 2 hours (I guess the equivalent of the agent having trained on the game) 

### Algorithm

the insight of deep q learning was to have 



